---
layout:     post
title:      "知识图谱「 5」"
subtitle:   " 从NLP中的词嵌入开始--word2vec详解 "
date:       2020-05-15 18:00:00
author:     "西山晴雪"
header-img: "img/post-bg-android.jpg"
categories: KnowledgeGraph
tags:
    - 知识图谱
    - 知识表示
    - 图模型 
    - 词嵌入
    - word2vec    
---
# word2vec详解



【摘要】本文翻译自Xin Rong的《word2vec Parameter Learning Explained》，是一篇非常优秀的英文论文。文章介绍了两种基本模型：CBOW和Skip-Gram模型的原理和求导的细节，之后介绍了优化模型的方法：分层softmax和负采样技术。



## **1 连续词袋模型（ $CBOW$ ）**

### **1.1 单个上下文的情境**（二元模型）

#### **（1）模型结构**		

 		我们先来介绍 $CBOW$ 模型中最简单的一个版本。假定每一上下文只有一个单词，也就是模型通过给定1个上下文单词来预测1个目标单词，就好比一个二元模型。图 1 展示了该模型的神经网络结构，是一个三层的前馈神经网络。其中：模型词汇表的大小是 $V$ ，隐藏层维度是 $N$ ，相邻神经元之间为全连接方式。

<img src="https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-52be7b29eb86ac796ef078d42e220f1e_720w.jpg" alt="img" style="zoom: 67%;" />



#### （2）模型前馈过程

- **模型输入层**
  - 长度为 $|V|$ 的独热（ $one-hot$ ）向量，用符号 $X$ 表示。
  - 独热向量：对于给定的单词，$\{x_1,x_2,..,x_V\}$ 序列中只有一个值为 $1$ ，其他值为 $0$ 。
- 模型输出层
  - 预测值：长度为 $|V|$ 的概率向量，向量中每个元素代表输入为 $w_i$（即 $x_i =1$ ）时，输出为  $w_j$ 的概率
- **输入层到隐藏层的转换**
  - 输入层和隐藏层之间的权重矩阵可以用一个 $|V| \times |N| $  的矩阵 $W$ 来表示。
  - $W$ 的每一行是长度为 $N$ 的向量  $v_w^T$ 。给定上下文（也就是一个单词），假定输入序列中 $x_k=1$和 $ x_k^{'}=0$ （对所有的 $ x_k^{'} \ne x_k$），可以得到：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-9dd65cd28858b00b99284afa0125c7f2_720w.png)

​		也就是直接复制 $W$ 矩阵第 $k$ 行赋给 $h$ ， $v_{w_i}$ 是输入单词 $w_i$ 的向量表示。这表明隐藏层单元的激活函数是简单的线性关系（也就是传递输入向量的加权和给下一层）。

- **隐藏层和输出层的转换**

  a）两者之间存在一个不同的矩阵 $W^{'}$ ，该矩阵的维度是  $N \times V$  。有了这些权重，我们可以计算出词汇表中每个单词  $u_{j}$ 的得分（  $v_{w_j}^{'}$ 是矩阵 $W^{'}$ 的第 $j$ 列）：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-46276488d7890eb133f801e5ae80b633_720w.png)

​         b）采用 $softmax$  计算输出单词的后验概率分布为（ $y_{j}$ 是输出向量中第 $j$ 个单元）：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-c5ecb93b3466b8215833f818658ed071_720w.png)

​		 c）把式（1）和式（2）带入式（3），可得：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-0e6b0ac9f7d58afd44603d21817aca5a_720w.jpg)

>注意：
>
> $v_{w}$ 和 $v_{w}^{'}$ 是对单词 $w$ 的两种不同的向量表示：
>
>-  $v_{w}$ 来自于矩阵 $W$ 的行向量， 通常将  $v_{w}$ 称为单词 $w$ 的输入向量（也称中心词向量）
>
>- $v_{w}^{'}$ 来自于矩阵 $W^{’}$ 的列向量，通常将  $v_{w}^{'}$ 称为单词 $w$ 的输出向量（也称上下文向量）。



#### **（3） 反向求导更新策略**

- **隐藏层 $\rightarrow$ 输出层的权重更新**

​		该模型的目标函数是使公式（4）最大化，即给定输入单词 $w_i$ 的条件下，所有单词的输出概率中，正确单词  $w_{O}$  （索引为  $j^{*}$ ）的条件概率最大。

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-1181ff28032ef62b3cded13d58947d06_720w.jpg)

​		损失函数  $-logP(w_{O}|w_{I})$ （我们的目标是最小化 $E$ ）， $j^{*}$ 是真实输出值的索引。该损失函数可以理解为一种特殊的衡量两个概率分布的交叉熵形式。现在我们来求解隐藏层和输出层权重更新方程，求 $E$ 对 $u_{j}$ 的偏导，可得：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-b6b651675575a25419a511a84e9e4ea0_720w.jpg)

当 $j=j^{*}$ 时， $ t_{j} =1$，否则为 $0$ 。可以看出，这个结果恰好等于**预测误差** $e_{j}$  。接下来对  $w_{ij}^{'}$ 求偏导，得到隐藏层 $\rightarrow$ 输出层权重的梯度：

![img](https://pic4.zhimg.com/80/v2-605fc7634cfc31a619c9de0d44635abf_720w.jpg)

因此，使用随机梯度下降的方法，我们可以得到隐藏层 $\rightarrow$ 输出层的权重更新公式：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-7d5edeb4e2e2a44fce8dec7d402806f5_720w.jpg)

学习率  $\eta >0 $，  $e_{j}y_{j}-t_{j}$  ，  $h_{j}$ 是隐藏层的第 $j$ 个单元； $v_{w_j}^{'}$ 是单词 $w_{j}$ 的输出向量。注意到该更新公式意味着必须遍历词汇表中的所有单词来才能得到输出值$y_i$ ，再将 $y_i$ 和期望输出 $t_j$（0或1）进行比对。如果 $y_{j}>t_{j}$ （结果估计过高），那么我们让 $v_{w_j}^{'}$ 减去一定比例的隐向量**h**（也就是 $w_{wi}$)，这样使得向量 $v_{wj}^{'}$  更加远离向量 $v_{wi}$  ；如果 $y_j<t_j$ (结果被低估，仅当 $t_j=1$ 时发生的情况，此时 $w_j=w_o$  )，我们加上一定比例的h给 $v_{wo}^{'}$ ，使得 $v_{wo}^{'}$ 更接近于 $v_{wi}^{'}$。如果 $y_{j}和t_{j}$ 非常接近，根据公式，权重的更新值也会很小。**再次提醒，输入向量**  $v_{w}) **和输出向量** $v_{w}^{'}$ **是对同一个单词w的两种不同的表达。**



- **输入层** $\rightarrow$ **隐藏层权重的更新公式**

​		有了 $W^{'}$ 的更新公式，我们继续求解 $W$ 的更新公式。我们求 $E$ 对 $h_{i}$ 的偏导，得到：


![img](https://pic2.zhimg.com/80/v2-96589f07873fdf41eef766c0bece2c79_720w.jpg)

 		 $h_{i}$ 是隐藏层第 $i$ 个单元的输出；根据公式（2）的定义， $u_{j}$ 是输出层的第 $j$ 个输入； $e_{j} y_{j}-t_{j}$是输出层第 $j$ 个单词的预测误差。$EH$ 是一个N维向量，是词汇表所有单词的输出向量根据预测误差的加权求和。

​		接下来，我们来求 $E$ 对输入权重矩阵 $W$ 的偏导。首先，让我们回忆一下，隐藏层是对输入层的值的线性计算过程。把公式（1）展开得到：

![img](https://pic1.zhimg.com/80/v2-43b035a244f5250dd9d194b08f920bd4_720w.jpg)

​		现在我们可以对 $W$ 的各元素进行求导，得到：

![img](https://pic1.zhimg.com/80/v2-64adb8c59541430bcd334b0d9d359fb8_720w.jpg)

​		这和 $x$ 和 $EH$ 的张量计算输出等价，

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-377fb93cd9f96769946952554eb40e63_720w.jpg)

​		我们最终得到了一个 $|V|*|N|$ 的矩阵。因为 $x$ 向量中只有一个元素是非0的，所有  $\frac{\partial E} {\partial W}$ 矩阵中只有一行是非0，该行的值为 $EH$ ( $x$ 的值为1)，是一个 $N$ 维向量。所以最终 $W$ 的更新公式为：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-58eb2f14ebbbef2daff177932441ddfd_720w.jpg)

 		$v_{wi}$ 是唯一上下文单词的输入向量，也是 $W$ 矩阵的导数中唯一非0的一行。W矩阵的其他行在迭代过程中均保持不变，因为他们的导数都为0。

​		直观的来理解一下这个过程，由于 $EH$ 向量是词汇表中所有单词的输出向量根据预测误差 $e_{j}%3Dy_{j}-t_{j}$的加权求和。我们可以这样理解公式（16），将输出向量的一部分加到上下文单词的输入向量中去。如果在输出层，单词  $w_{j}$ 作为预测单词的概率被高估 $y_{j}>t_{j}$，那么上下文单词 $w_i$ 的输入向量趋向于远离 $w_{j}$ 的输出向量；相反，如果单词 $w_{j}$ 作为预测单词的概率被低估 $y_{j}<t_{j}$,那么上下文单词 $w_i$ 的输入向量将趋向于靠近 $w_{j}$ 的输出向量；如果单词 $w_{j}$ 作为预测单词恰好等于真实值，那么上下文单词 $w_i$ 的输入向量将几乎没有什么改变。输入向量 $w_i$ 的变化情况取决于词汇表中所有单词向量的预测误差；误差越大，输入向量的变动越大。

​		由于在训练过程中，我们是通过训练语料中的 “上下文--目标单词” 对来迭代地更新模型参数，每次迭代更新对向量的影响也是累积的。我们可以想象成单词 $w$ 的输出向量被 $w$ 所有周围词的输入向量的来回往复的拖拽。就好比有真实的琴弦在单词 $w$ 和其他词之间。同样的，输入向量也可以被想象成被很多输出向量拖拽。这种解释可以提醒我们想象成一个重力，或者其他力量所指引的输出图。每根弦的均衡长度跟共同出现的关联单词对之间的力量权衡有关，也跟学习率有关。经过多次迭代，输入和输出向量的相对位置将趋于平稳。

### **1.2 多个单词的上下文情境**（多元模型）

#### （1）模型结构		

​		图2展示了多个上下文单词的 $CBOW$ 模型。当计算隐藏层时， $CBOW$ 并不是直接拷贝上下文的输入向量，而是对上下文单词的输入向量求均值后输出h。

![img](https://pic4.zhimg.com/80/v2-12b76690d3a57fd9301a7f2eb671643b_720w.jpg)

C是上下文的单词个数， $w_{1} \cdot \cdot \cdot w_{C}$ 是上下文中的单词， $v_{w}$ 是单词 $w$ 的输入向量。损失函数定义为：

![img](https://pic2.zhimg.com/80/v2-fa4e707ac56bcf5963db065ac193a181_720w.jpg)

​	跟公式（7）一样，目标和one-word-context模型一致，除了h的定义不同，也就是用公式（18）替换公式（1）。

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-73885c25dd5d7cbbba9e7834b18c0aa3_720w.jpg)

#### （2）反向梯度与更新策略

- 对于隐藏层 --> 输出层
  - 梯度更新公式跟one-word-context模型一致。直接拷贝公式如下：

![img](https://pic3.zhimg.com/80/v2-4c3955565ca5baf71795631c473eae26_720w.jpg)

   注意一点在训练过程中我们需要把该公式迭代作用于输出矩阵的每个元素。

- 对于输入层 --> 隐藏层
  - 梯度更新公式和式（16）类似，除了需要将如下公式作用于上下文词的每个单词 $w_{I,c}$ ：

![img](https://pic4.zhimg.com/80/v2-3903ac953b04b387d1f62e0ea2add353_720w.jpg)    

​     $w_{I,c}$ 是上下文中第C个单词的输入向量； $\eta$ 是学习率； $EH$ 由公式（12）得到。对更新公式的直观理解可以参考（16）。



## **2. Skip-Gram模型**

​		$Skip-Gram$ 模型是由 $Mikolov$ 等人提出的。图3展示了 $Skip-Gram$ 模型的过程。该模型可以看做是 $CBOW$ 模型的逆过程。 $CBOW$ 模型的目标单词在该模型中作为输入，上下文则作为输出。

​		我们仍然使用  $v_{wi}$ 来表示输入层中唯一单词（也叫中心词）的输入向量，所以这样的话，对隐藏层 $h$ 的定义跟公式（1）一致，意味着 $h$ 仅仅只是简单拷贝了输入层-->隐藏层的权重矩阵 $W$ 中跟输入单词 $wi$ 相关的那一行。拷贝公式得到：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-d44fb4fcafadac7d3d2874390ac45c34_720w.jpg)

​	

​	在输出层，我们输出C个多项式分布来替代仅输出一个多项式分布。每个输出是由同一个隐藏层--> 输出层的矩阵计算得出的：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-e67131a4c545ab7064e08d258bba569c_720w.jpg)

​		这里 $w_{c,j}$ 是第c个输出上的第 $j$ 个单词； $w_{O,c}$ 是中心词对应的目标单词中的第c个单词； $wi$ 是中心词（唯一输入单词）； $y_{c,j}$ 是第c个输出面上第 $j$ 个单元的输出值； $u_{c,j} $ 是第c个输出上的第 $j$ 个单元的输入。因为输出共享同一权重矩阵，所以有：

![img](https://pic2.zhimg.com/80/v2-180be00c6ad4e502c221c567e9a283c9_720w.jpg)

   $v_{wj}^{'}$ 是词汇表第 $j$ 个单词的输出向量，可由  $W^{'}$ j矩阵中的所对应的一列拷贝得到。

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-0ed8b9efc8aa7867fbf001b921cec4ee_720w.jpg)

​	模型参数的更新过程和one-word-context模型基本一致。损失函数更改为：

![img](https://pic3.zhimg.com/80/v2-d7b6b1b841299b6f7e037d0b05e1731a_720w.jpg)

 $j_{c}^{*}$ 是词汇表中第c个真实输出单词的索引。我们求 $E$ 对 $u_{c,j}$的偏导，得到：

![img](https://pic1.zhimg.com/80/v2-87f14a487cf6db59c65d23ddf17f6b64_720w.jpg)

跟公式（8）一致，这个就是该单元的预测误差。为了方便表示，我们定义一个 $V$ 维向量 $EI= \left\{EI_{1} \cdot\cdot\cdot EI_{V}\right\}$ ，该向量是C个预测单词的误差总和：

![img](https://pic3.zhimg.com/80/v2-4eab584e418132b7f4282584adec8cf2_720w.jpg)

接下来，我们作 $E$ 对  $W^{'}$ （隐藏层 --> 输出层的权重矩阵）的偏导，可得：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-7fcd21a077d87bda49599e1e9b4ed4d3_720w.jpg)

这样我们得到了隐藏层到输出层矩阵 $W^{'}$ 的梯度更新公式，

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-c63d9610b3e7068c72bb91813a017447_720w.jpg)

直观上对该公式的理解跟（11）一样，除了预测误差是对输出层所有上下文词的误差总和。

对输入层 --> 隐藏层的权重更新跟（12）和（16）一致，除了需要将预测误差  $e_{j}^{}$替换为 $EI_{j}^{}$ ，直接给出更新公式：

![img](https://pic2.zhimg.com/80/v2-c1ac11335701c64c8157b1d4ca41cd79_720w.jpg)

$EH$ 是一个 $N$ 维向量，向量每个单元被定义为：

![img](https://pic2.zhimg.com/80/v2-12dbb71018d739dc0e13cc48cb3fb611_720w.jpg)

直觉上的理解和（16）一致。



## **3 模型的优化方法**

​		以上我们讨论的模型（CBOW和skip-gram）都是他们的原始形式，没有加入任何优化技巧。对于这些模型，每个单词存在两类向量表达：输入向量 $v_{w}^{}$ 、输出向量 $v_{w}^{'} $ （这也是为什么word2vec的名称由来：1个单词对应2个向量表示）。

​		学习得到输入向量比较简单；但要学习输出向量是很困难的。从公式（22）和（33）得知，为了更新 $v_{w}^{'} $ ，在每次训练中，我们必须遍历词汇表中的每个单词 $w_{j}^{} $ ，从而计算得到 $u_j$ ，预测概率  $y_{j}^{}$（skip-gram为 $y_{c,j}$ )、它们的预测误差 $e_j$ ，（skip-gram为 $EI_{j}^{}$ ），然后再用误测误差来更新输出向量 $v_{j}^{'}$ 。

​		对每个训练过程做如此庞大的计算是非常昂贵的，使得它难以扩展到词汇表或者训练样本很大的任务中去。为了解决这个问题，直观的想法就是限制每次必须更新的输出向量的数量。一种有效的手段就是采用**分层softmax**；另一种可行的方法是通过**负采样**。

​		这两种技巧都只针对**输出向量**更新的优化。在我们的推导过程中，我们关心三种值：

- $E$ ，新的目标函数；
-   $\frac{ \partial E}{\partial v_{w}^{'}}$  ，新的输出向量的更新公式；
-  $\frac{\partial E}{ \partial h}$，反向传播中用于更新输入向量的预测误差的加权和。



### **3.1 分层softmax**

​		分层softmax是计算softmax问题的一种有效的方法。该模型用二叉树来表示词汇表中的所有单词。 $|V|$ 个单词必须存储于二叉树的叶子单元。可以证明共有 $V-1$ 个非叶子单元。对于每个叶子节点，有一条唯一的路径可以从根节点到达该叶子节点；该路径被用来计算该叶子结点所代表的单词的概率。参考图4：

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-758cbf836d38e953700fbece2e62dd4b_720w.jpg)

图4：用于分层softmax的二叉树示例。白色节点表示词汇表中的所有单词，黑色节点表示隐节点。从根节点到单词  $w_{2}$的路径被加粗表示出，在这个例子中，该路径的长度 $L(w_{2} ) = 4$ 。 $n(w_2,j)$ 表示从根节点到单词的路径上第 $j$  个节 点。分层softmax模型**没有单词的输出向量**，取而代之的是，  $V-1 $ 个隐节点都有一个输出向量  $v_{n(w,j)}^{'}$  。一个单词作为输出词的概率被定义为：

![img](https://pic3.zhimg.com/80/v2-c07b6b2feaa7b762edcf6f0b9e5902b2_720w.jpg)

$ch(n)$ 是节点  $n$  的左侧子节点；![[公式]](https://www.zhihu.com/equation?tex=v_{n(w%2Cj)}^{'})是隐节点 ![[公式]](https://www.zhihu.com/equation?tex=n(w%2Cj)) 的向量表示（“输出向量”）； ![[公式]](https://www.zhihu.com/equation?tex=h) 是隐藏层的输出值（skip-gram模型中，h= ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwi%7D) ；CBOW模型中，h= ![[公式]](https://www.zhihu.com/equation?tex=1%2FC\sum_{c%3D1}^{C}{v_{w_{c}}}) ）;

![[公式]](https://www.zhihu.com/equation?tex=[[x]]) 是一个特殊的函数，定义如下：



![img](https://pic1.zhimg.com/80/v2-f4b6a6effe6b41e9385975d93f4c5390_720w.jpg)



让我们通过一个例子来直观上理解一下这个公式。看图4，假如我们想要计算![[公式]](https://www.zhihu.com/equation?tex=w_{2}) 作为输出单词的概率。我们将这个概率定义为问题中从根节点出发到叶子结点的随机路径。在每个隐节点（包含根节点），我们需要分配往左走或往右走的概率。我们定义在当前隐节点 ![[公式]](https://www.zhihu.com/equation?tex=n) 往左走的概率为：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-309ec9bc8a5fa39cb3b27047bb05be5f_720w.jpg)



它是由隐节点向量和隐藏层输出值（ ![[公式]](https://www.zhihu.com/equation?tex=h) ，也就是输入单词的向量表示）共同决定。容易得到，从隐节点 ![[公式]](https://www.zhihu.com/equation?tex=n) 往右走的概率为：



![img](https://pic1.zhimg.com/80/v2-40c5f62fead84436efdedd67dcd8f634_720w.jpg)



如图4，沿着从根节点到单词![[公式]](https://www.zhihu.com/equation?tex=w_{2})的路径，我们可以计算![[公式]](https://www.zhihu.com/equation?tex=w_%7B2%7D)是输出单词的概率为：



![img](https://pic1.zhimg.com/80/v2-9bc7a6f0bd3eb29bdaf389d7030a1abc_720w.jpg)



也就是公式（37）。不难看出：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-18f12a41cfd04f494cd02fdd1939d759_720w.jpg)



这使得分层softmax模型是一个的关于所有单词的良好定义的多分类分布。

现在我们来推导隐节点向量的参数更新公式。为了简化，我们先从one-word-context模型开始。再扩展到CBOW和skip-gram模型就会很容易了。为了表达简单，我们使用下面的简化公式：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-99a163f0fe2683bb68584e4debb51030_720w.jpg)





![img](https://pic1.zhimg.com/80/v2-2d502865ad0cf47c5f929b99f475a404_720w.jpg)



对于训练过程，我们将误差函数定义为：



![img](https://pic2.zhimg.com/80/v2-038f8b92d8e62cef0721b847c499a65d_720w.jpg)



我们作E对 ![[公式]](https://www.zhihu.com/equation?tex=v_{j}^{'}h) 的偏导，可得：



![img](https://pic2.zhimg.com/80/v2-c70e72c3eb91fe593cb2e5a68098d69d_720w.jpg)



当 ![[公式]](https://www.zhihu.com/equation?tex=[[\bullet]]) =1时， ![[公式]](https://www.zhihu.com/equation?tex=t_%7Bj%7D)= 1，当![[公式]](https://www.zhihu.com/equation?tex=%5B%5B%5Cbullet%5D%5D)=-1时，![[公式]](https://www.zhihu.com/equation?tex=t_%7Bj%7D)= 0。

接下来我们求 ![[公式]](https://www.zhihu.com/equation?tex=E) 对隐节点 ![[公式]](https://www.zhihu.com/equation?tex=n(w%2Cj)) 的偏导，可得：



![img](https://pic3.zhimg.com/80/v2-55590d73b6f065c242e8fec923116b1a_720w.jpg)



最终我们的梯度更新公式为：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-d76c3ee6b2b11ef725638f97bc50e1fc_720w.jpg)



该公式会从 ![[公式]](https://www.zhihu.com/equation?tex=j%3D1%2C2%2C\cdot\cdot\cdot%2C+L(w)-1) 依次迭代。我们可以将 ![[公式]](https://www.zhihu.com/equation?tex=\sigma(v_{j}^{'T}h_{})-t_{j}) 理解隐节点

![[公式]](https://www.zhihu.com/equation?tex=n(w%2Cj)) 的预测误差。对每个隐节点来说，他们的任务就是预测下一步是该往左侧子节点走，还是往右侧子节点走。 ![[公式]](https://www.zhihu.com/equation?tex=t_{j}%3D1) 表示实际路径是往左；![[公式]](https://www.zhihu.com/equation?tex=t_{j}%3D0)表示实际路径是往右。

![[公式]](https://www.zhihu.com/equation?tex=\sigma(v_{j}^{'T}h_{}))表示预测结果。对于训练数据来说，如果对隐节点的预测结果和真实路径非常相似，那么它的向量 ![[公式]](https://www.zhihu.com/equation?tex=v_{j}^{'}) 只需要微小的改动；否则，向量![[公式]](https://www.zhihu.com/equation?tex=v_%7Bj%7D%5E%7B%27%7D)就会按适当的方向进行调整（要么靠近，要么远离 ![[公式]](https://www.zhihu.com/equation?tex=h) ）来减少预测误差。这个更新公式既可以用于CBOW模型，也可以用于skip-gram模型。当用于skip-gram模型时，需要对输出的C个单词中的每个词重复这个更新过程。

我们通过对 ![[公式]](https://www.zhihu.com/equation?tex=E) 求 ![[公式]](https://www.zhihu.com/equation?tex=h) 的偏导来看误差在输入层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 隐藏层的反向传播过程：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-5ed56f05c2cf28293d38d5b96987388b_720w.jpg)



这个结果可以直接带入公式（23）来求得CBOW模型的输入向量的更新公式。对于skip-gram模型，我们可以计算窗口的每个单词的 ![[公式]](https://www.zhihu.com/equation?tex=EH) 值，对这些 ![[公式]](https://www.zhihu.com/equation?tex=EH) 值加和后再带入公式（35）来得到输入向量的更新公式。

从更新公式可以看出，训练模型的计算复杂度从 ![[公式]](https://www.zhihu.com/equation?tex=O(V)) 降至 ![[公式]](https://www.zhihu.com/equation?tex=O(logV)) ，这在效率上是一个巨大的提升。而且我们仍然有差不多同样的模型参数（原始模型： ![[公式]](https://www.zhihu.com/equation?tex=V) 个单词的输出向量，分层softmax： ![[公式]](https://www.zhihu.com/equation?tex=V-1) 个隐节点的输出向量）。



### **3.2 负采样技术**

相比分层softmax，负采样的思想更加直观：为了解决数量太过庞大的输出向量的更新问题，我们就不更新全部向量，而只更新他们的一个样本。

显然正确的输出单词（也就是正样本）应该出现在我们的样本中，另外，我们需要采集几个单词作为负样本（因此该技术被称为“负采样”）。采样的过程需要指定总体的概率分布，我们可以任意选择一个分布。我们把这个分布叫做噪声分布，标记为 ![[公式]](https://www.zhihu.com/equation?tex=P_{n}(w)) 。可以凭经验选择一个好的分布。

在word2vec中，作者称用简化的训练目标取代用一个定义好的后多项分布的负采样形式，也能够产生高质量的词嵌入：



![img](https://pic3.zhimg.com/80/v2-386883fda481d89d17198ab112e28ae6_720w.jpg)





![[公式]](https://www.zhihu.com/equation?tex=w_{O}) 是输出单词（即，正样本）， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwo%7D%5E%7B%27%7D) 是它的词向量； ![[公式]](https://www.zhihu.com/equation?tex=h) 是隐藏层的输出：在CBOW中，

h= ![[公式]](https://www.zhihu.com/equation?tex=1%2FC\sum_{c%3D1}^{C}{v_{w_{c}}})，在skip-gram中， ![[公式]](https://www.zhihu.com/equation?tex=h%3Dv_{wi}) ； ![[公式]](https://www.zhihu.com/equation?tex=W_{neg}%3D\left\{+w_{j}+|j%3D1%2C\cdot\cdot\cdot%2C+K\right\}) 是 从

![[公式]](https://www.zhihu.com/equation?tex=P_{n}(w)) 中采样得到的单词集合，也就是负样本。

为了得到基于负采样的词向量的更新公式，我们首先求 ![[公式]](https://www.zhihu.com/equation?tex=E) 对输出单元 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)

的网络输入的偏导：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-4c198c8e6ccabedb14fafc52478695cd_720w.jpg)





![[公式]](https://www.zhihu.com/equation?tex=t_%7Bj%7D)是单词![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)的标签。t=1时， ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)是正样本；t=0时，![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)为负样本。

接下来我们求 ![[公式]](https://www.zhihu.com/equation?tex=E) 对单词![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)的输出向量求偏导，



![img](https://pic3.zhimg.com/80/v2-681863bed27724ee8f544424b1ae316a_720w.jpg)



由此可以得到输出向量的更新公式：



![img](https://pic4.zhimg.com/80/v2-718370f9ec46a74d8018cb5cdc11bc73_720w.jpg)



只需要将此公式作用于 ![[公式]](https://www.zhihu.com/equation?tex=w_{j}\in\left\{+w_{O}+\right\}\cup+W_{neg}) 而不用更新词汇表的所有单词。这也解释了为什们我们可以在一次迭代中节省巨大的计算量。

直觉上对该更新公式的理解和公式（11）一致。该公式可以通用于CBOW模型和skip-gram模型。对于skip-gram模型，我们一次作用于一个上下文单词。

为了使误差反向传播到隐藏层来更新单词的输入向量，我们求 ![[公式]](https://www.zhihu.com/equation?tex=E) 对隐藏层输出 ![[公式]](https://www.zhihu.com/equation?tex=h) 的偏导：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-813bce9b891ada8aaba204e535a8601e_720w.jpg)



把 ![[公式]](https://www.zhihu.com/equation?tex=EH) 带入公式（23）可得CBOW模型的输入向量的更新公式。对于skip-gram模型，我们计算每个单词的![[公式]](https://www.zhihu.com/equation?tex=EH)值并加和再带入公式（35）就可得到输入向量的更新公式。的输出向量求偏导，



![img](https://pic3.zhimg.com/80/v2-681863bed27724ee8f544424b1ae316a_720w.jpg)



由此可以得到输出向量的更新公式：



![img](https://pic4.zhimg.com/80/v2-718370f9ec46a74d8018cb5cdc11bc73_720w.jpg)



只需要将此公式作用于 ![[公式]](https://www.zhihu.com/equation?tex=w_{j}\in\left\{+w_{O}+\right\}\cup+W_{neg}) 而不用更新词汇表的所有单词。这也解释了为什们我们可以在一次迭代中节省巨大的计算量。

直觉上对该更新公式的理解和公式（11）一致。该公式可以通用于CBOW模型和skip-gram模型。对于skip-gram模型，我们一次作用于一个上下文单词。

为了使误差反向传播到隐藏层来更新单词的输入向量，我们求 ![[公式]](https://www.zhihu.com/equation?tex=E) 对隐藏层输出 ![[公式]](https://www.zhihu.com/equation?tex=h) 的偏导：



![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-813bce9b891ada8aaba204e535a8601e_720w.jpg)



把 ![[公式]](https://www.zhihu.com/equation?tex=EH) 带入公式（23）可得CBOW模型的输入向量的更新公式。对于skip-gram模型，我们计算每个单词的![[公式]](https://www.zhihu.com/equation?tex=EH)值并加和再带入公式（35）就可得到输入向量的更新公式。