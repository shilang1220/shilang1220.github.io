---
layout:     post
title:      "知识图谱「 4 」"
subtitle:   " 为什么要做图嵌入（Graph Embedding）？ "
date:       2020-06-30 09:00:00
author:     "西山晴雪"
header-img: "img/post-bg-android.jpg"
categories: KnowledgeGraph
tags:
    - 知识图谱
    - 知识表示
    - 图模型 
    - 图嵌入
    - Graph Embedding
---
# 为什么要进行图嵌入？

------

​	**【摘要】**图模型存在于真实世界的广泛场景中。例如：社交网络中的人及其联系、生物蛋白质及其作用、通信网络IP地址及其通信等。此外，常见的图片、句子也可抽象为图模型。因此，图模型可以说是无处不在。 基于图模型可以解决 节点分类、链接预测、聚类、可视化、推理等很多应用中的实际问题，例如：社交网络中新关系的预测、生物分子中蛋白质功能和相互作用的预测、通信网络中异常事件的预测等。传统图模型采用“One-hot变量+邻接矩阵”的方式来表示图结构，数据维度高、计算复杂度高，对于下游任务的效率和实现影响非常大。图嵌入正是对图模型进行表达的一种新方法，而且在实际研究和应用中被证明为一种非常有效的技术。

## **1. 什么是图嵌入（graph embedding）？**

​		图嵌入是一种将图数据（通常为高维稀疏矩阵）映射为低纬度稠密向量的过程（如下图），其目标是发现高维图的低维向量表示。图嵌入需要捕捉到图的拓扑结构、顶点与顶点的关系、以及其他的信息 （如子图，连边等）。如果有更多的信息被表示出来，那么下游的任务将会获得更好的表现。在嵌入的过程中存在着一种共识：向量空间中保持连接的节点彼此靠近。基于此，研究者提出了拉普拉斯特征映射（Laplacian Eigenmaps）和局部线性嵌入（Locally Linear Embedding ，LLE）。

![img](https://pic3.zhimg.com/80/v2-f8614f72c10b871f8321d07888df971e_720w.jpg)

​		图嵌入技术大致可以分为两种：

- **节点嵌入：**
  - 仅对节点进行分类、相似度预测、分布可视化时一般采用节点嵌入，典型如：自然语言处理中的子嵌入、词嵌入。
- **图嵌入：**
  - 在图级别上进行预测和推理时，需要将整个图表示为一个向量进行嵌入表示。

## **2. 为什么要使用图嵌入**（graph embedding）**？**

​	出于下面的原因需要对图进行嵌入表示：

- **传统图模型大多是采用符号表示方法，缺乏语义**
  - 典型如：翠微饭店和京西宾馆同样是酒店，但在邻接矩阵中两者之间毫无关系。
- **在图上直接进行机器学习具有一定的局限性**
  - 图是由节点和边构成的数据结构，而机器学习的输入通常使用数学、统计量等表示形式，通过图嵌入可以将图模型以向量空间的方式表示，从而适应机器学习的表示要求。
- **图嵌入能够压缩数据** 
  - 传统用邻接矩阵的方法描述图中节点间的连接，当图中节点数非常大时（如：100万个节点），邻接矩阵是一个100万 X 100万的超大稀疏矩阵，既难以存储、下游任务计算效率又低。而图嵌入可以视为一种图压缩技术，能够起到降维的作用。
  -  连接矩阵的维度是|V| x |V|，其中|V| 是图中节点的个数。矩阵中的每一列和每一行都代表一个节点。矩阵中的非零值表示两个节点已连接。将邻接矩阵用用大型图的特征空间几乎是不可能的。一个具有1M节点和1Mx1M的邻接矩阵的图该怎么表示和计算呢？但是嵌入可以看做是一种压缩技术，能够起到降维的作用。
- **向量计算比直接在图上操作更加的简单、快捷**

## 3. 图嵌入的重要原则

- **保留重要的图属性或特征**
  - 确保嵌入能够很好地描述图的属性，如：图拓扑、节点连接关系、节点领域关系等，这样后期任务才能获得较好的表现。
- **可扩展性和效率**
  - 大多数真实网络都很大，包含了大量节点和边。嵌入方法应具有可扩展性，能够处理大型图。
  - 定义一个可扩展模型具有挑战性，尤其是当该模型旨在保持网络的全局属性时。
  - 网络大小不应降低嵌入速度，好的嵌入方法应当既能够在子图上高效嵌入，也能够在大图上高效嵌入。
- **合理的嵌入维度**
  - 实际嵌入时很难找到表示的最佳维数
  - 维度越大能够保留的信息越多，但有更高的时间和空间复杂度
  - 较低的维度虽然时间、空间复杂度低，但会损失很多图中原有的信息

## **4. 节点嵌入方法**

### 4.1 节点嵌入的基本原理--幂次法则

​	幂次法则即“80-20”法则，即在任何一组食物中，最重要的仅占其中一小部分，其余的尽管是多数，但却是次要的。典型如：语料库中单词的分布、社交网络中的节点。

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-ae71898a11a0651345fe13ee44ff6ce1_720w.jpg)

### 4.2 NLP中的节点嵌入--word2vec

#### 4.2.1 概述	

自然语言中的词显著地遵循幂次法则，即重要的词在数量上占少数，但是在频度（重要性）上占多数。同时，不同词之间，存在着语义相关性（如：同时出现的频度等）。因此，自然语言中的词嵌入，可以作为节点嵌入的一种特殊形式。随着神经网络技术的发展，利用海量语料通过非监督的神经网络方法获取自然语言词嵌入表示模型，已经成为目前自然语言处理领域的热点。

​	其中，具有里程碑意义的方法包括：

- word2vec
- ELMo
- BERT
- GPT等

#### 4.2.2 一个基本假设

- 分布式假设：分布相似的词语具有相似的语义，或者说“相似的词应具有相似的嵌入”

#### 4.2.3 两种模型

- Skip-Gram模型
  - 1:1、1:多，已知输入单词，预测上下文中单词出现的概率
- CBOW模型
  - 多：1，已知上下文单词，预测中心单词出现的概率

#### 4.2.4 Skip-Gram模型

- 这是具有一层隐藏层的神经网络（总共三层）。 skip-gram模型是给出某一词语来预测上下文相邻的单词。下图显示了输入单词（标有绿色）和预测单词的示例。 通过此任务，作者实现了两个相似的词具有相似的嵌入，因为具有相似含义的两个词可能具有相似的邻域词。

![img](https://pic4.zhimg.com/80/v2-4dc804ba7ecd32a001f1f7ead03bfeb3_720w.jpg)

下图是skip-gram模型。网络的输入为one-hot编码，长度与单词字典的长度相同，只有一个位置为1，输出为单词的嵌入表示。

![img](https://pic4.zhimg.com/80/v2-22db53554e0cde59c9a16ffa1aa3577f_720w.jpg)图片来自【9】

#### 4.2.5 CBOW模型



下面介绍三个节点嵌入（node embedding）的方法，一个图嵌入(graph embedding)的方法，这些方法都类似Word2vec【3】的嵌入原理。



### **4.4 图节点的嵌入方法（Node embeddings）**

下面介绍三种经典的节点嵌入的方法：DeepWalk【2】, Node2vec【8】, SDNE【7】

3.1.1 DeepWalk

DeepWalk通过**随机游走**(truncated random walk)学习出一个网络的**表示**，在网络标注顶点很少的情况也能得到比较好的效果。随机游走起始于选定的节点，然后从当前节点移至随机邻居，并执行一定的步数，该方法大致可分为三个步骤：

- **采样**：通过随机游走对图上的节点进行采样，在给定的时间内得到一个节点构成的序列，论文研究表明从每个节点执行32到64次随机遍历就足够表示节点的结构关系；
- **训练skip-gram**：随机游走得到的节点序列与word2vec方法中的句子相当。文本中skip-gram的输入是一个句子，在这里输入为随机游走采样得到的序列，进一步通过最大化预测相邻节点的概率进行预测周围节点进行训练和学习。 通常预测大约20个邻居节点-左侧10个节点，右侧10个节点。
- **计算嵌入**

![img](https://pic2.zhimg.com/80/v2-5f3178156b3c9593cc45ba6a13ec2701_720w.jpg)图片来自【9】

DeepWalk通过随机游走去可以获图中点的局部上下文信息，因此学到的表示向量反映的是该点在图中的局部结构，两个点在图中共有的邻近点（或者高阶邻近点）越多，则对应的两个向量之间的距离就越短。但是DeepWalk方法随机执行随机游走，这意味着嵌入不能很好地保留节点的局部关系，Node2vec方法可以解决此问题。

3.1.2 Node2vec

Node2vec是DeepWalk的改进版，定义了一个bias random walk的策略生成序列，仍然用skip gram去训练。

该算法引入了参数P和Q，参数Q关注随机游走中未发现部分的可能性，即控制着游走是向外还是向内: 若Q>1，随机游走倾向于访问接近的顶点(偏向BFS); 若Q<1，倾向于访问远离的顶点(偏向DFS)。

参数P控制了随机游走返回到前一个节点的概率。也就是说，参数P控制节点局部关系的表示，参数Q控制较大邻域的关系。

![img](https://raw.githubusercontent.com/shilang1220/imageBed/master/img/v2-fe47adeb0ed5d07701285594d26c9de8_720w.jpg)图片来自【9】

3.1.3 SDNE

SDNE没有采用随机游走的方法而是使用自动编码器来同时优化一阶和二阶相似度，学习得到的向量表示保留局部和全局结构，并且对稀疏网络具有鲁棒性。一阶相似度表征了边连接的成对节点之间的局部相似性。 如果网络中的两个节点相连，则认为它们是相似的。 举个例子：当一篇论文引用另一篇论文时，意味着它们很可能涉及相似的主题，如6和7。但是当两个节点不相连时如5和6，他们就不具有相似度了吗？显然不是，从图上可以看出来他们虽然没有直接连接，但是他们有共同的邻居1,2,3,4,那么这时候就需要用二街相似度来衡量了。

![img](https://pic4.zhimg.com/80/v2-6e5bb9e75ec8073093db06abffd569bf_720w.jpg)图片来自【9】

二阶相似度表示节点邻域结构的相似性，它能够了表征全局的网络结构。 如果两个节点共享许多邻居，则它们趋于相似。

> SDNE的具体做法是使用自动编码器来保持一阶和二阶网络邻近度。它通过联合优化这两个近似值来实现这一点。该方法利用高度非线性函数来获得嵌入。模型由两部分组成：无监督和监督。前者包括一个自动编码器，目的是寻找一个可以重构其邻域的节点的嵌入。后者基于拉普拉斯特征映射，当相似顶点在嵌入空间中彼此映射得很远时，该特征映射会受到惩罚

![img](https://pic3.zhimg.com/80/v2-77229cc867228bb9016f2a559c103b82_720w.jpg)图片来自【9】

![img](https://pic2.zhimg.com/80/v2-efc9754d0737a18a8eeb60308b60bccd_720w.jpg)图片来自【7】，提出的模型由无监督部分与监督部分两部分组成。无监督部分用于提取与二阶估计相关的特征；同时由于部分节点是直接相连的,因而可以得到其的一阶估计,从而引入模型的有监督部分。这两部分的损失函数线性组合,联合优化

## **5. 图嵌入方法**

> 关于整个图嵌入的方式这里介绍具有代表性的graph2vec

图嵌入是将整个图用一个向量表示的方法，Graph2vec【5】同样是基于skip-gram思想，把整个图编码进向量空间, 类似文档嵌入doc2vec, doc2vec在输入中获取文档的ID，并通过最大化文档预测随机单词的可能性进行训练。

![img](https://pic4.zhimg.com/80/v2-e8715c9841d67b0a88abdee81adf38ef_720w.jpg)图片来自【9】

Graph2vec同样由三个步骤构成

- 采样并重新标记图中的所有子图。子图是出现在所选节点周围的一组节点。子图中的节点距离所选边数不远。
- 训练skip-gram模型。 类似于文档doc2vec, 由于文档是单词集，而图是子图集，在此阶段，对skip-gram模型进行训练。经过训练，可以最大程度地预测输入中存在于图中的子图的概率。

![img](https://pic2.zhimg.com/80/v2-a8d8a23d6bb1989cf6b27e296588bf51_720w.jpg)图片来自【10】



- 通过在输入处提供子图的id索引向量来计算嵌入

## **3.3其他的方法**

以上提到了四个常用的方法，但是除此之外还有非常多的方法和模型值得学习

- **Node embedding**: LLE, Laplacian Eigenmaps, Graph Factorization, GraRep, HOPE, DNGR, GCN, LINE
- **Graph embedding** approaches: Patchy-san, sub2vec (embed subgraphs), WL kernel andDeep WL kernels

## **总结**

本文介绍了Graph embedding是什么，为什么要采用Graph embedding，以及进行embedding时需要满足的性质。进一步简单介绍了node-level的三个经典的方法：DeepWalk, node2vec, SDNE以及graph-levle的graph2vec，下期将梳理GCN, GAT, GraphSage, GIN等工作，欢迎关注。



参考

[1] C. Manning, R. Socher, *Lecture 2 | Word Vector Representations: word2vec* (2017), *[YouTube](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DERibwqs9p38)*.

[2] B. Perozzi, R. Al-Rfou, S. Skiena, DeepWalk: Online Learning of Social Representations (2014), *[arXiv:1403.6652](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1403.6652)*.

[3] C. McCormick. [Word2Vec Tutorial — The Skip-Gram Model](https://link.zhihu.com/?target=http%3A//mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) (2016), *[http://mccormickml.com](https://link.zhihu.com/?target=http%3A//mccormickml.com/).*

[4] T. Mikolov, K. Chen, G. Corrado, J. Dean, Efficient Estimation of Word Representations in Vector Space (2013), *[arXiv:1301.3781](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1301.3781)*.

[5] A. Narayanan, M. Chandramohan, R. Venkatesan, L. Chen, Y. Liu, S. Jaiswal, graph2vec: Learning Distributed Representations of Graphs (2017),
*[arXiv:1707.05005](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.05005).*

[6] P. Goyal, E. Ferrara, *Graph Embedding Techniques, Applications, and Performance: A Survey (2018),* *[Knowledge-Based Systems](https://link.zhihu.com/?target=https%3A//www.sciencedirect.com/science/article/abs/pii/S0950705118301540).*

[7] D. Wang, P. Cui, W. Zhu, Structural Deep Network Embedding (2016), *[Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining.](https://link.zhihu.com/?target=https%3A//dl.acm.org/citation.cfm%3Fid%3D2939753)*

[8] A. Grover, J. Leskovec, node2vec: Scalable Feature Learning for Networks (2016), *[Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining.](https://link.zhihu.com/?target=https%3A//dl.acm.org/citation.cfm%3Fid%3D2939754)*

[9] [https://towardsdatascience.com/graph-embeddings-the-summary-cc6075aba007](https://link.zhihu.com/?target=https%3A//towardsdatascience.com/graph-embeddings-the-summary-cc6075aba007)

[10] [https://www.groundai.com/project/graph2vec-learning-distributed-representations-of-graphs/1](https://link.zhihu.com/?target=https%3A//www.groundai.com/project/graph2vec-learning-distributed-representations-of-graphs/1)

发布于 2019-10-20